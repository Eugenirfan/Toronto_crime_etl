###File download: - Use below to read data as a df

base_url = "https://ckan0.cf.opendata.inter.prod-toronto.ca"
 
# Datasets are called "packages". Each package can contain many "resources"
# To retrieve the metadata for this package and its resources, use the package name in this page's URL:
url = base_url + "/api/3/action/package_show"
params = { "id": "major-crime-indicators"}
package = requests.get(url, params = params).json()
df= pd.read_csv(package['result']['resources'][3]['url'])

###ssl errors for Mac users: - import below to avoid any ssl errors
import ssl
ssl._create_default_https_context = ssl._create_unverified_context

###Project Setup

###GCP Setup
GCP is organized around projects. You may create a project and access all available GCP resources and services from the project dashboard.

Create a project and a service account, and download authentication keys to your local machine:

* Create an account on GCP. You should receive $300 in credit when signing up on GCP for the first time with an account.
* Setup a new project and write down the Project ID.
* From the GCP Dashboard, click on the drop down menu next to the Google Cloud Platform title to show the project list and click on New project.
* Give the project a name. We will use dtc-de in this example. You can use the autogenerated Project ID (this ID must be unique to all of GCP, not just your account). Leave the organization as No organization. Click on Create.
* Back on the dashboard, make sure that your project is selected. Click on the previous drop down menu to select it otherwise.
* Setup a service account for this project and download the JSON authentication key files.
* IAM & Admin > Service accounts > Create service account
* Provide a service account name. We will use dtc-de-user. Leave all other fields with the default values. Click on Create and continue.
* Grant the Viewer role (Basic > Viewer) to the service account and click on Continue
* There is no need to grant users access to this service account at the moment. Click on Done.
* With the service account created, click on the 3 dots below Actions and select Manage keys.
* Add key > Create new key. Select JSON and click Create. The files will be downloaded to your computer. Save them to a folder and write down the path.
* Download the GCP SDK for local setup. Follow the instructions to install and connect to your account and project.
* Set the environment variable to point to the auth keys.
* The environment variable name is GOOGLE_APPLICATION_CREDENTIALS
* The value for the variable is the path to the json authentication file you downloaded previously.
* In the following chapters we will setup a Data Lake on Google Cloud Storage and a Data Warehouse in BigQuery. We will explore these concepts in future lessons but a Data Lake is where we would usually store data and a Data Warehouse provides a more structured way to access this data.
* Assign the following IAM Roles to the Service Account: Storage Admin, Storage Object Admin, BigQuery Admin and Viewer.
* On the GCP Project dashboard, go to IAM & Admin > IAM
* Select the previously created Service Account and edit the permissions by clicking on the pencil shaped icon on the left.
* Add the following roles and click on Save afterwards:
* Storage Admin: for creating and managing buckets.
* Storage Object Admin: for creating and managing objects within the buckets.
* BigQuery Admin: for managing BigQuery resources and data.
* Viewer should already be present as a role.
* Enable APIs for the project (these are needed so that Terraform can interact with GCP):
https://console.cloud.google.com/apis/library/iam.googleapis.com
https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com
Make sure that the GOOGLE_APPLICATION_CREDENTIALS environment variable is set.

###Orchestration - Prefect:

###download below using requirements.txt

pandas==1.5.2
prefect==2.7.7
prefect-sqlalchemy==0.2.2
prefect-gcp[cloud_storage]==0.2.3
protobuf==4.21.11
pyarrow==10.0.1
pandas-gbq==0.19.0

###After reading the DF, upload it to GCS (DataLake) Using Prefect or Airflow:

###Create required blocks in prefect orion for GCS and Bigquery 

@task()
def write_gcs(crime_df: pd.DataFrame) -> None:
    """Upload local parquet file to GCS"""
    gcs_bucket = GcsBucket.load("zoomcampfinal")
    gcs_bucket.upload_from_dataframe(df=crime_df,to_path="data.csv", serialization_format='csv')
    return
    
    
 Write it to BQ using below prefect task:
 @task()
def write_bq(df: pd.DataFrame) -> None:
    """Write DataFrame to BiqQuery"""

    gcp_credentials_block = GcpCredentials.load("zoom-gcp-creds")

    df.to_gbq(
        destination_table="crime_data_all.crime_data",
        project_id="final-project-zoomcamp",
        credentials=gcp_credentials_block.get_credentials_from_service_account(),
        if_exists="append",
    )
    
###Transformation:
Connect to bq using dbt and write transformations and create models for queries


###Containerization:
All above could be done in docker when ran on a VM instance as gcs to bq requires us to download the file locally first.


